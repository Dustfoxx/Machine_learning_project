{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying positive or negative based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\" in tweet}) = \\frac{P(\"\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\"\\text{\"good\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we need the probability of \"good\" being in a positive tweet, the probability of a tweet being positive and the probability of \"good\" being in a tweet. \n",
    "\n",
    "Similarly if we want to obtain the opposite \"is this a negative tweet given that it contains the word \"boring\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"negative\"}|\\text{\"boring\" in tweet}) = \\frac{P(\\text{\"boring\" in tweet}|\\text{\"negative\"})P(\\text{\"negative\"})}{P(\\text{\"boring\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"boring\" being in a negative tweet, the probability of a tweet negative being and the probability of \"boring\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"positive\"|\"good\" in tweet) $>$ P(\"negative\"|\"boring\" in tweet)\n",
    "    \n",
    "   Tweet is positive\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is negative\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theroem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a positive tweet given that it contains the word \"good\" and \"interesting\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\", \"interesting\" in tweet}) = \\frac{P(\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"interesting\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\\text{\"good\" in tweet})P(\\text{\"interesting\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that you will be working with can be downloaded from the following link: https://uppsala.instructure.com/courses/66466/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:33:05.384466Z",
     "start_time": "2021-01-21T13:33:05.376779Z"
    }
   },
   "outputs": [],
   "source": [
    "#stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data, explore and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:18.885001Z",
     "start_time": "2021-01-21T13:30:18.675615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets=pd.read_csv('twitter_sentiment_analysis.csv',encoding='latin', \n",
    "                   names = ['sentiment','id','date','query','user','tweet'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200000, 6)\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.sample(frac=1)\n",
    "tweets = tweets[:200000]\n",
    "print(\"Dataset shape:\", tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Currently (0 = negative and 4 = positive) changing the notation to (0 = negative and 1 = positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198847</th>\n",
       "      <td>1</td>\n",
       "      <td>1985216080</td>\n",
       "      <td>Sun May 31 16:39:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PaperCakes</td>\n",
       "      <td>@MissAmyO I love those deer notebooks! I've wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721849</th>\n",
       "      <td>0</td>\n",
       "      <td>2261316939</td>\n",
       "      <td>Sat Jun 20 20:36:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>abc_christ</td>\n",
       "      <td>@ddlovato awww thats sad y would any1 want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535207</th>\n",
       "      <td>1</td>\n",
       "      <td>2178956235</td>\n",
       "      <td>Mon Jun 15 08:26:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>MarcyChen</td>\n",
       "      <td>@willfrancis Kind of cool and overcast, but su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170944</th>\n",
       "      <td>1</td>\n",
       "      <td>1980433540</td>\n",
       "      <td>Sun May 31 06:43:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jordantng</td>\n",
       "      <td>is thinking of many people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58065</th>\n",
       "      <td>0</td>\n",
       "      <td>1685816992</td>\n",
       "      <td>Sun May 03 02:14:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>realhelen</td>\n",
       "      <td>AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845408</th>\n",
       "      <td>1</td>\n",
       "      <td>1564219238</td>\n",
       "      <td>Mon Apr 20 02:23:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Charlenecollins</td>\n",
       "      <td>Good morning ppl  Hoping to get rid of this Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381853</th>\n",
       "      <td>1</td>\n",
       "      <td>2052407062</td>\n",
       "      <td>Sat Jun 06 00:37:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>faceurfears</td>\n",
       "      <td>@CopTheTruth Thanks....You to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461009</th>\n",
       "      <td>1</td>\n",
       "      <td>2063891685</td>\n",
       "      <td>Sun Jun 07 04:28:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JaneFated</td>\n",
       "      <td>@dyles_ftw Goodnight dyanne! Happy Wedding! Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988255</th>\n",
       "      <td>1</td>\n",
       "      <td>1834747536</td>\n",
       "      <td>Mon May 18 04:04:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>briannedixon</td>\n",
       "      <td>i feel refreshed and useful. time for some pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384853</th>\n",
       "      <td>1</td>\n",
       "      <td>2052694756</td>\n",
       "      <td>Sat Jun 06 01:39:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sugarman65</td>\n",
       "      <td>@RefinanceMyHome I love your flashing Sphere, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "1198847          1  1985216080  Sun May 31 16:39:57 PDT 2009  NO_QUERY   \n",
       "721849           0  2261316939  Sat Jun 20 20:36:06 PDT 2009  NO_QUERY   \n",
       "1535207          1  2178956235  Mon Jun 15 08:26:38 PDT 2009  NO_QUERY   \n",
       "1170944          1  1980433540  Sun May 31 06:43:54 PDT 2009  NO_QUERY   \n",
       "58065            0  1685816992  Sun May 03 02:14:04 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "845408           1  1564219238  Mon Apr 20 02:23:21 PDT 2009  NO_QUERY   \n",
       "1381853          1  2052407062  Sat Jun 06 00:37:32 PDT 2009  NO_QUERY   \n",
       "1461009          1  2063891685  Sun Jun 07 04:28:51 PDT 2009  NO_QUERY   \n",
       "988255           1  1834747536  Mon May 18 04:04:10 PDT 2009  NO_QUERY   \n",
       "1384853          1  2052694756  Sat Jun 06 01:39:47 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1198847       PaperCakes  @MissAmyO I love those deer notebooks! I've wa...  \n",
       "721849        abc_christ  @ddlovato awww thats sad y would any1 want to ...  \n",
       "1535207        MarcyChen  @willfrancis Kind of cool and overcast, but su...  \n",
       "1170944        jordantng                       is thinking of many people!   \n",
       "58065          realhelen        AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE   \n",
       "...                  ...                                                ...  \n",
       "845408   Charlenecollins  Good morning ppl  Hoping to get rid of this Mo...  \n",
       "1381853      faceurfears                     @CopTheTruth Thanks....You to   \n",
       "1461009        JaneFated  @dyles_ftw Goodnight dyanne! Happy Wedding! Yo...  \n",
       "988255      briannedixon  i feel refreshed and useful. time for some pro...  \n",
       "1384853       sugarman65  @RefinanceMyHome I love your flashing Sphere, ...  \n",
       "\n",
       "[200000 rows x 6 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiment']=tweets['sentiment'].replace(4,1)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198847</th>\n",
       "      <td>1</td>\n",
       "      <td>@MissAmyO I love those deer notebooks! I've wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721849</th>\n",
       "      <td>0</td>\n",
       "      <td>@ddlovato awww thats sad y would any1 want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535207</th>\n",
       "      <td>1</td>\n",
       "      <td>@willfrancis Kind of cool and overcast, but su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170944</th>\n",
       "      <td>1</td>\n",
       "      <td>is thinking of many people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58065</th>\n",
       "      <td>0</td>\n",
       "      <td>AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69352</th>\n",
       "      <td>0</td>\n",
       "      <td>whywhywhywhywhymeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794669</th>\n",
       "      <td>0</td>\n",
       "      <td>i miss @sarahar092 and @marianna527  i need to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228493</th>\n",
       "      <td>0</td>\n",
       "      <td>Ugg boots don't salsa well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563666</th>\n",
       "      <td>0</td>\n",
       "      <td>@Ascasewwen Was the tea good tea? That would b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472595</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning twetterheads! Time to make this d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet\n",
       "1198847          1  @MissAmyO I love those deer notebooks! I've wa...\n",
       "721849           0  @ddlovato awww thats sad y would any1 want to ...\n",
       "1535207          1  @willfrancis Kind of cool and overcast, but su...\n",
       "1170944          1                       is thinking of many people! \n",
       "58065            0        AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE \n",
       "69352            0                               whywhywhywhywhymeee \n",
       "794669           0  i miss @sarahar092 and @marianna527  i need to...\n",
       "228493           0                        Ugg boots don't salsa well \n",
       "563666           0  @Ascasewwen Was the tea good tea? That would b...\n",
       "1472595          1  Good morning twetterheads! Time to make this d..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.drop(['date','query','user'], axis=1, inplace=True)\n",
    "tweets.drop('id', axis=1, inplace=True)\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if any null values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0.0\n",
       "tweet        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tweets.isnull().sum() / len(tweets))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now make a new column for side by side comparison of new tweets vs old tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting pandas object to a string type\n",
    "tweets['tweet'] = tweets['tweet'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the number of positive vs. negative tagged sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         200000\n",
      "No. of positve tagged sentences is:  99826\n",
      "No. of negative tagged sentences is: 100174\n"
     ]
    }
   ],
   "source": [
    "positives = tweets['sentiment'][tweets.sentiment == 1 ]\n",
    "negatives = tweets['sentiment'][tweets.sentiment == 0 ]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(tweets.shape[0]))\n",
    "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Fox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Fox\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourself', 'hasn', 'has', 'him', 'its', 'all', 'shouldn', \"it's\", \"wouldn't\", 'and', 'from', 'can', 'between', \"you're\", 'then', 'ma', 'an', 'most', 'didn', \"couldn't\", \"shan't\", \"you'll\", \"that'll\", 'weren', 'more', 'such', 'nor', 'own', 'will', \"hasn't\", 'you', 'if', 'as', 'than', 'have', 'further', 'against', 'any', 't', 'it', 'hadn', 'too', 'but', 'itself', 'themselves', 'because', 'above', 'needn', \"didn't\", 'until', 'are', 'been', 'each', 'how', \"you've\", 'only', 'ain', 'mustn', 'wasn', 'this', 'to', 'aren', 'myself', \"mustn't\", 'herself', 'mightn', 'did', 'before', 'y', 'won', 'there', 'some', 'in', 'other', 'about', 'on', 'our', 'that', 'so', 'which', \"won't\", 'now', 'my', 's', 'with', 'under', 'is', 'be', \"should've\", 'where', \"mightn't\", 'who', 'the', 'he', 'out', 'again', 'theirs', \"don't\", 'while', 'once', 'they', 'we', 'i', 'below', 're', \"shouldn't\", \"aren't\", 'a', 'during', 'what', 'or', 'these', 'had', 'd', 'doesn', 'yours', 'o', \"weren't\", 'having', 'when', 'into', \"haven't\", 'his', 'those', 'through', 'don', \"wasn't\", 'yourselves', 'their', 'doing', 'for', 'off', 'm', 'was', 'whom', \"she's\", 'hers', 'your', 'both', 'should', 'll', 'here', 'does', 'by', 'after', 'do', 'why', 'up', 've', 'just', 'of', 'she', \"hadn't\", 'were', 'being', 'over', 'them', 'at', 'not', 'same', 'wouldn', 'her', \"isn't\", 'few', 'isn', 'himself', 'me', 'very', \"doesn't\", 'shan', 'ourselves', 'no', 'ours', 'haven', 'am', \"needn't\", 'down', \"you'd\", 'couldn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Fox\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) \n",
    "#that a search engine has been programmed to ignore,\n",
    "#both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "some = 'amp,today,tomorrow,going,girl'\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \n",
    "    \n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
    "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
    "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
    "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
    "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
    "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
    "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
    "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
    "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
    "    tweet = re.sub(r\"hahah|hahaha|hahahaha\", \"haha\", tweet)\n",
    "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
    "    tweet = re.sub(r\"thanx|thnx\", \"thanks\", tweet)\n",
    "    tweet = re.sub(r\"goood\", \"good\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet=tweet[1:]\n",
    "    # Removing all URls \n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet) \n",
    "    #remove some words\n",
    "    tweet= re.sub(some,'',tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #tokens = [w for w in tokens if len(w)>2]\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem \n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", \n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "     \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abbrev_in_text(tweet):\n",
    "    t=[]\n",
    "    words=tweet.split()\n",
    "    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n",
    "    return ' '.join(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text processing completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198847</th>\n",
       "      <td>1</td>\n",
       "      <td>@MissAmyO I love those deer notebooks! I've wa...</td>\n",
       "      <td>missamyo love deer notebook wanted since chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721849</th>\n",
       "      <td>0</td>\n",
       "      <td>@ddlovato awww thats sad y would any1 want to ...</td>\n",
       "      <td>ddlovato awww thats sad would any1 want brake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535207</th>\n",
       "      <td>1</td>\n",
       "      <td>@willfrancis Kind of cool and overcast, but su...</td>\n",
       "      <td>willfrancis kind cool overcast supposed warm a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170944</th>\n",
       "      <td>1</td>\n",
       "      <td>is thinking of many people!</td>\n",
       "      <td>thinking many people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58065</th>\n",
       "      <td>0</td>\n",
       "      <td>AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE</td>\n",
       "      <td>aahhhhh signal tv gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845408</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning ppl  Hoping to get rid of this Mo...</td>\n",
       "      <td>ood morning people hoping get rid monday morni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381853</th>\n",
       "      <td>1</td>\n",
       "      <td>@CopTheTruth Thanks....You to</td>\n",
       "      <td>copthetruth thanksyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461009</th>\n",
       "      <td>1</td>\n",
       "      <td>@dyles_ftw Goodnight dyanne! Happy Wedding! Yo...</td>\n",
       "      <td>dylesftw goodnight dyanne happy wedding enjoy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988255</th>\n",
       "      <td>1</td>\n",
       "      <td>i feel refreshed and useful. time for some pro...</td>\n",
       "      <td>feel refreshed useful time productivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384853</th>\n",
       "      <td>1</td>\n",
       "      <td>@RefinanceMyHome I love your flashing Sphere, ...</td>\n",
       "      <td>refinancemyhome love flashing sphere neat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet  \\\n",
       "1198847          1  @MissAmyO I love those deer notebooks! I've wa...   \n",
       "721849           0  @ddlovato awww thats sad y would any1 want to ...   \n",
       "1535207          1  @willfrancis Kind of cool and overcast, but su...   \n",
       "1170944          1                       is thinking of many people!    \n",
       "58065            0        AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE    \n",
       "...            ...                                                ...   \n",
       "845408           1  Good morning ppl  Hoping to get rid of this Mo...   \n",
       "1381853          1                     @CopTheTruth Thanks....You to    \n",
       "1461009          1  @dyles_ftw Goodnight dyanne! Happy Wedding! Yo...   \n",
       "988255           1  i feel refreshed and useful. time for some pro...   \n",
       "1384853          1  @RefinanceMyHome I love your flashing Sphere, ...   \n",
       "\n",
       "                                          processed_tweets  \n",
       "1198847  missamyo love deer notebook wanted since chris...  \n",
       "721849   ddlovato awww thats sad would any1 want brake ...  \n",
       "1535207  willfrancis kind cool overcast supposed warm a...  \n",
       "1170944                               thinking many people  \n",
       "58065                               aahhhhh signal tv gone  \n",
       "...                                                    ...  \n",
       "845408   ood morning people hoping get rid monday morni...  \n",
       "1381853                              copthetruth thanksyou  \n",
       "1461009  dylesftw goodnight dyanne happy wedding enjoy ...  \n",
       "988255             feel refreshed useful time productivity  \n",
       "1384853          refinancemyhome love flashing sphere neat  \n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['processed_tweets'] = tweets['tweet'].apply(lambda x: process_tweets(x))\n",
    "tweets['processed_tweets'] = tweets['processed_tweets'].apply(lambda x: convert_abbrev_in_text(x))\n",
    "print('Text Preprocessing complete.')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1198847</th>\n",
       "      <td>1</td>\n",
       "      <td>@MissAmyO I love those deer notebooks! I've wa...</td>\n",
       "      <td>missamyo love deer notebook wanted since chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721849</th>\n",
       "      <td>0</td>\n",
       "      <td>@ddlovato awww thats sad y would any1 want to ...</td>\n",
       "      <td>ddlovato awww thats would any1 want brake hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535207</th>\n",
       "      <td>1</td>\n",
       "      <td>@willfrancis Kind of cool and overcast, but su...</td>\n",
       "      <td>willfrancis kind cool overcast supposed warm a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170944</th>\n",
       "      <td>1</td>\n",
       "      <td>is thinking of many people!</td>\n",
       "      <td>thinking many people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58065</th>\n",
       "      <td>0</td>\n",
       "      <td>AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE</td>\n",
       "      <td>aahhhhh signal gone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet  \\\n",
       "1198847          1  @MissAmyO I love those deer notebooks! I've wa...   \n",
       "721849           0  @ddlovato awww thats sad y would any1 want to ...   \n",
       "1535207          1  @willfrancis Kind of cool and overcast, but su...   \n",
       "1170944          1                       is thinking of many people!    \n",
       "58065            0        AAAHHHHH!!!! THE SIGNAL ON THE TV HAS GONE    \n",
       "\n",
       "                                          processed_tweets  \n",
       "1198847  missamyo love deer notebook wanted since chris...  \n",
       "721849   ddlovato awww thats would any1 want brake hear...  \n",
       "1535207  willfrancis kind cool overcast supposed warm a...  \n",
       "1170944                               thinking many people  \n",
       "58065                                  aahhhhh signal gone  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing shortwords\n",
    "tweets['processed_tweets']=tweets['processed_tweets'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-21T13:30:22.469848Z",
     "start_time": "2021-01-21T13:30:22.450611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198847    missamyo love deer notebook wanted since chris...\n",
      "721849     ddlovato awww thats would any1 want brake hear...\n",
      "1535207    willfrancis kind cool overcast supposed warm a...\n",
      "1170944                                 thinking many people\n",
      "58065                                    aahhhhh signal gone\n",
      "                                 ...                        \n",
      "1182078                                  lyricalgangsta cute\n",
      "302589     theangelforever well overnight although swear ...\n",
      "584036                   itting around family beach housesad\n",
      "1393679    cpayan happy bday dday mean different thing st...\n",
      "1052271    jennom hard time usually theme naturally come ...\n",
      "Name: processed_tweets, Length: 150000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweets_data = tweets[\"processed_tweets\"]\n",
    "tweets_labels = tweets[\"sentiment\"]\n",
    "\n",
    "#Split data into train_tweets, test_tweets, train_labels and test_labels\n",
    "\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets_data, tweets_labels, shuffle=False)\n",
    "print(train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of positive tweet\" P(pos) , \"probability of negative tweet\" P(neg), \"probability of word in tweet given tweet is positive\" P(w|pos) and \"probability of word in tweet given tweet is negative\" P(w|neg). Start by calculating the probability that a tweet is positive and negative respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total length of the data is:         200000\n",
    "#No. of positve tagged sentences is:  100191\n",
    "#No. of negative tagged sentences is: 99809\n",
    "\n",
    "P_pos = 100191/200000\n",
    "P_neg = 99809/200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|pos), P(w|neg) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance {'good': 'Pos':150, 'Neg': 10} meaning good occurs in 150 positive tweets and 10 negative tweets. Be aware that we are not interested in calculating multiple occurances of the same word in the same tweet. Also we change the labels from 0 for \"Negative\" and 1 for \"Positive\" to \"Neg\" and \"Pos\" respectively.For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 4889, 'Neg': 2446}\n"
     ]
    }
   ],
   "source": [
    "new_train_labels = train_labels.replace(0, \"Neg\", regex=True)\n",
    "final_train_labels = new_train_labels.replace(1, \"Pos\", regex=True)\n",
    "word_counter = {} \n",
    "for (tweet, label) in zip(train_tweets, final_train_labels):\n",
    "    tweet_words = tweet.split()\n",
    "    for i in range(len(tweet_words)):\n",
    "        tweet_words[i] = tweet_words[i].lower()\n",
    "    tweet_words = set(tweet_words)\n",
    "    for word in tweet_words:\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = {\"Pos\": 0, \"Neg\": 0}    \n",
    "        word_counter[word][label] += 1\n",
    "# ... Count number of tweets each word occurs in and store in word_counter where an entry looks like ex. {'word': 'Pos':98, 'Neg':10}\n",
    "print(word_counter[\"good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with a smaller subset of words just to save up some time. Find the 1500 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'like', 'today', 'love', 'work', 'time', 'laughing', 'loud', 'going', 'know', 'back', 'really', 'want', 'night', 'think', 'need', 'still', 'well', 'thanks', 'would', 'much', 'home', 'miss', 'make', 'tomorrow', 'hope', 'feel', 'great', 'last', 'though', 'haha', 'morning', 'twitter', 'wish', 'week', 'come', 'sleep', 'right', 'thing', 'could', 'tonight', 'friend', 'sorry', 'people', 'hour', 'happy', 'better', 'look', 'nice', 'wait', 'yeah', 'school', 'hate', 'getting', 'even', 'thank', 'awesome', 'show', 'weekend', 'next', 'dont', 'soon', 'take', 'never', 'life', 'little', 'long', 'year', 'tweet', 'cant', 'first', 'everyone', 'watch', 'movie', 'best', 'girl', 'sick', 'tired', 'already', 'always', 'find', 'please', 'something', 'suck', 'working', 'sure', 'feeling', 'phone', 'watching', 'done', 'ready', 'cool', 'made', 'keep', 'song', 'hurt', 'pretty', 'away', 'another', 'help', 'house', 'thought', 'ever', 'oing', 'went', 'till', 'sound', 'start', 'mean', 'summer', 'game', 'amazing', 'looking', 'maybe', 'left', 'early', 'guess', 'later', 'someone', 'baby', 'lost', 'missed', 'tell', 'glad', 'hear', 'coming', 'party', 'weather', 'rain', 'follow', 'actually', 'stuff', 'also', 'play', 'trying', 'birthday', 'live', 'head', 'hard', 'nothing', 'since', 'excited', 'call', 'talk', 'give', 'around', 'might', 'said', 'many', 'world', 'friday', 'luck', 'thats', 'exam', 'bored', 'atching', 'monday', 'yesterday', 'late', 'music', 'video', 'must', 'damn', 'follower', 'found', 'stop', 'sunday', 'gone', 'read', 'finally', 'leave', 'beautiful', 'anything', 'cold', 'least', 'check', 'cute', 'food', 'month', 'waiting', 'wanted', 'making', 'believe', 'hair', 'enjoy', 'poor', 'forward', 'book', 'real', 'outside', 'funny', 'dinner', 'picture', 'lunch', 'family', 'without', 'free', 'place', 'everything', 'class', 'mine', 'cause', 'shit', 'woke', 'welcome', 'probably', 'saturday', 'facebook', 'idea', 'wrong', 'sweet', 'every', 'anymore', 'thinking', 'room', 'anyone', 'enough', 'iphone', 'message', 'almost', 'minute', 'playing', 'stay', 'missing', 'didnt', 'came', 'update', 'stupid', 'coffee', 'following', 'dream', 'post', 'money', 'finished', 'totally', 'name', 'fuck', 'seen', 'okay', 'face', 'beach', 'hell', 'rest', 'whole', 'lovely', 'half', 'crazy', 'course', 'else', 'sooo', 'brother', 'blog', 'busy', 'problem', 'send', 'word', 'able', 'full', 'plan', 'kind', 'wont', 'learned', 'part', 'kinda', 'meet', 'headache', 'seems', 'trip', 'computer', 'listening', 'forgot', 'either', 'etting', 'about', 'hopefully', 'link', 'used', 'true', 'took', 'mother', 'ticket', 'heart', 'taking', 'city', 'email', 'mind', 'final', 'office', 'awww', 'super', 'change', 'wake', 'raining', 'news', 'photo', 'person', 'hehe', 'annot', 'sister', 'heard', 'break', 'started', 'reading', 'site', 'online', 'internet', 'rock', 'pain', 'seeing', 'hahaa', 'alone', 'shopping', 'quite', 'talking', 'care', 'drink', 'favorite', 'open', 'called', 'eating', 'remember', 'season', 'reply', 'tried', 'stuck', 'told', 'fine', 'using', 'lucky', 'concert', 'anyway', 'instead', 'fucking', 'broke', 'breakfast', 'sunny', 'bring', 'text', 'study', 'mileycyrus', 'drive', 'appy', 'boring', 'turn', 'laugh', 'seriously', 'walk', 'watched', 'dude', 'hello', 'leaving', 'asleep', 'hand', 'afternoon', 'june', 'shower', 'story', 'hungry', 'loved', 'starting', 'goodnight', 'happened', 'move', 'inally', 'reason', 'sleeping', 'wonderful', 'train', 'water', 'write', 'tommcfly', 'point', 'fall', 'jealous', 'moment', 'died', 'laptop', 'bought', 'ride', 'bday', 'direct', 'together', 'running', 'fail', 'high', 'awake', 'second', 'crap', 'perfect', 'couple', 'album', 'meeting', 'visit', 'soooo', 'dead', 'cream', 'mood', 'hang', 'foot', 'line', 'close', 'page', 'definitely', 'ipod', 'finish', 'list', 'smile', 'lady', 'town', 'award', 'test', 'clean', 'holiday', 'church', 'store', 'happen', 'seem', 'worry', 'episode', 'account', 'ddlovato', 'sore', 'knew', 'oday', 'hoping', 'interesting', 'sometimes', 'sigh', 'youtube', 'tour', 'broken', 'catch', 'star', 'dear', 'evening', 'goin', 'homework', 'longer', 'side', 'short', 'mister', 'chance', 'sunshine', 'pick', 'hank', 'congrats', 'orning', 'rying', 'question', 'worth', 'supposed', 'park', 'listen', 'sitting', 'worst', 'window', 'gave', 'fast', 'nite', 'weird', 'throat', 'agree', 'number', 'three', 'chocolate', 'unfortunately', 'past', 'parent', 'date', 'dance', 'studying', 'scared', 'answer', 'enjoying', 'forget', 'moving', 'wonder', 'rather', 'doesnt', 'understand', 'istening', 'sent', 'followfriday', 'website', 'horrible', 'team', 'whats', 'pool', 'wedding', 'body', 'slow', 'tuesday', 'black', 'english', 'babe', 'comment', 'card', 'earlier', 'london', 'flight', 'project', 'shoe', 'beat', 'company', 'light', 'sleepy', 'jonas', 'green', 'vote', 'thursday', 'math', 'kill', 'cheer', 'upset', 'load', 'huge', 'college', 'shop', 'fair', 'aking', 'special', 'forever', 'dress', 'worse', 'loving', 'driving', 'easy', 'meant', 'blue', 'finger', 'vacation', 'eading', 'eally', 'support', 'saying', 'beer', 'lazy', 'looked', 'service', 'moon', 'writing', 'inside', 'kidding', 'sadly', 'orking', 'wear', 'paper', 'cake', 'save', 'woman', 'learn', 'figure', 'application', 'cousin', 'chicken', 'small', 'band', 'tweeting', 'heading', 'especially', 'except', 'airport', 'order', 'google', 'possible', 'miley', 'shot', 'apple', 'case', 'annoying', 'july', 'chat', 'hospital', 'stomach', 'eeling', 'wednesday', 'aving', 'boyfriend', 'fell', 'shall', 'different', 'sign', 'road', 'mate', 'apparently', 'garden', 'white', 'david', 'york', 'hows', 'myspace', 'voice', 'touch', 'safe', 'plus', 'turned', 'warm', 'cleaning', 'camera', 'needed', 'wondering', 'drinking', 'ight', 'club', 'liked', 'itting', 'nother', 'front', 'what', 'sims', 'living', 'slept', 'proud', 'scary', 'lonely', 'vega', 'yummy', 'doctor', 'join', 'exciting', 'twilight', 'realized', 'bitch', 'killing', 'shame', 'taken', 'bike', 'film', 'nope', 'lose', 'played', 'fact', 'gorgeous', 'smell', 'rainy', 'worked', 'glass', 'goodbye', 'graduation', 'state', 'spend', 'aiting', 'pizza', 'hubby', 'happens', 'havent', 'ooking', 'issue', 'french', 'download', 'felt', 'behind', 'share', 'power', 'absolutely', 'radio', 'yall', 'wine', 'officially', 'hill', 'spent', 'ating', 'exactly', 'guitar', 'shirt', 'sold', 'business', 'decided', 'mile', 'father', 'lame', 'deal', 'shoot', 'wishing', 'event', 'isnt', 'giving', 'everybody', 'indeed', 'others', 'group', 'type', 'laying', 'hanging', 'sale', 'tummy', 'storm', 'history', 'whatever', 'clothes', 'version', 'random', 'buddy', 'posted', 'choice', 'john', 'asked', 'xoxo', 'matter', 'door', 'single', 'taste', 'pink', 'prime', 'sing', 'near', 'alot', 'along', 'hold', 'appreciate', 'hotel', 'terrible', 'country', 'bird', 'death', 'street', 'round', 'lately', 'note', 'mommy', 'fixed', 'blood', 'walking', 'revision', 'fantastic', 'bloody', 'wife', 'closed', 'couldnt', 'nose', 'ache', 'child', 'tear', 'puppy', 'lakers', 'trouble', 'hangover', 'peace', 'background', 'ball', 'extra', 'bummed', 'ouch', 'issing', 'daughter', 'minister', 'hahaha', 'nail', 'although', 'cheese', 'quick', 'peep', 'somewhere', 'camp', 'plane', 'freaking', 'staying', 'degree', 'passed', 'jonasbrothers', 'drop', 'changed', 'interview', 'wearing', 'profile', 'dark', 'completely', 'none', 'alright', 'usually', 'channel', 'bless', 'mall', 'wasnt', 'drunk', 'sexy', 'kick', 'screen', 'traffic', 'sort', 'singing', 'hilarious', 'silly', 'self', 'system', 'ahead', 'ring', 'track', 'mail', 'aint', 'fire', 'badly', 'record', 'australia', 'happening', 'kiss', 'teeth', 'ahhh', 'training', 'angel', 'itunes', 'dying', 'worried', 'daddy', 'practice', 'upload', 'caught', 'floor', 'blackberry', 'dunno', 'info', 'disappointed', 'oodnight', 'enjoyed', 'adam', 'price', 'joke', 'memory', 'taylor', 'relaxing', 'color', 'cook', 'confused', 'stick', 'suppose', 'dancing', 'pissed', 'stopped', 'mention', 'swimming', 'serious', 'major', 'currently', 'speak', 'trailer', 'quoti', 'bill', 'chill', 'lesson', 'nick', 'usual', 'island', 'middle', 'burnt', 'stand', 'session', 'view', 'count', 'design', 'nobody', 'onna', 'area', 'mama', 'nearly', 'result', 'sooooo', 'young', 'demi', 'fight', 'doubt', 'pray', 'irst', 'future', 'awful', 'heck', 'piece', 'afraid', 'before', 'review', 'there', 'calling', 'fish', 'report', 'gift', 'milk', 'fever', 'husband', 'roll', 'france', 'contact', 'tweetdeck', 'reat', 'series', 'thru', 'quotthe', 'laundry', 'mark', 'planning', 'ello', 'present', 'straight', 'packing', 'neck', 'ishes', 'american', 'doin', 'failed', 'ended', 'bank', 'often', 'ishing', 'ored', 'energy', 'freakin', 'gettin', 'nervous', 'quiet', 'twice', 'release', 'available', 'kitty', 'sell', 'keeping', 'double', 'south', 'unless', 'soup', 'trek', 'asking', 'paid', 'sending', 'noticed', 'prob', 'exhausted', 'match', 'starbucks', 'dropped', 'heat', 'kate', 'congratulation', 'teacher', 'honey', 'blast', 'shake', 'twittering', 'sweetie', 'brain', 'offer', 'article', 'spending', 'germany', 'feelin', 'board', 'mess', 'invite', 'shut', 'midnight', 'swine', 'quot', 'shift', 'pack', 'telling', 'awwww', 'four', 'secret', 'become', 'showing', 'chicago', 'copy', 'bummer', 'excellent', 'chris', 'mobile', 'player', 'ahaha', 'tonite', 'killed', 'shout', 'positive', 'everyday', 'library', 'just', 'adorable', 'imagine', 'return', 'sometime', 'cried', 'gotten', 'cover', 'bunch', 'studio', 'finding', 'cell', 'gosh', 'race', 'complete', 'dentist', 'buying', 'sense', 'lake', 'losing', 'somebody', 'depressed', 'waking', 'anyways', 'updated', 'wrote', 'folk', 'begin', 'evil', 'gunna', 'bright', 'station', 'stage', 'inished', 'donniewahlberg', 'everywhere', 'moved', 'witter', 'important', 'travel', 'bath', 'knee', 'epic', 'relax', 'promise', 'spring', 'rule', 'original', 'step', 'block', 'eeds', 'expensive', 'server', 'followed', 'afford', 'tweeps', 'cancelled', 'medium', 'mouth', 'gutted', 'experience', 'machine', 'throw', 'button', 'spot', 'strawberry', 'given', 'talked', 'headed', 'empty', 'battery', 'thunder', 'fake', 'davidarchie', 'west', 'leaf', 'perhaps', 'celebrate', 'client', 'deserve', 'cost', 'burn', 'putting', 'search', 'girlfriend', 'king', 'code', 'essay', 'soccer', 'fresh', 'anywhere', 'loss', 'ampamp', 'checked', 'space', 'form', 'weight', 'character', 'wall', 'fave', 'crash', 'watchin', 'brought', 'youre', 'normal', 'allowed', 'checking', 'disney', 'bottle', 'quote', 'prom', 'annoyed', 'mcfly', 'topic', 'hero', 'depressing', 'hmmm', 'lack', 'million', 'festival', 'local', 'kitchen', 'delicious', 'bear', 'retweet', 'hannah', 'crappy', 'personal', 'magic', 'stress', 'brazil', 'finale', 'tune', 'seat', 'five', 'feed', 'eaving', 'barely', 'simple', 'tree', 'skin', 'winter', 'letting', 'strong', 'tough', 'boat', 'falling', 'however', 'america', 'picked', 'hurting', 'oves', 'market', 'njoying', 'surprise', 'helping', 'social', 'ending', 'bite', 'conference', 'brilliant', 'flower', 'assignment', 'jordanknight', 'shining', 'keyboard', 'omeone', 'pant', 'style', 'august', 'talent', 'shes', 'crashed', 'cooky', 'favourite', 'ugly', 'desk', 'size', 'youll', 'across', 'addicted', 'swear', 'nyone', 'sharing', 'salad', 'danny', 'connection', 'jonathanrknight', 'north', 'chip', 'fter', 'building', 'michael', 'grandma', 'control', 'canada', 'grade', 'apartment', 'tweeted', 'outta', 'spanish', 'hahaaha', 'james', 'awhile', 'added', 'twit', 'craving', 'fabulous', 'fucked', 'difficult', 'ired', 'image', 'couch', 'sport', 'voted', 'stayed', 'blame', 'grad', 'messed', 'surgery', 'xbox', 'florida', 'wave', 'credit', 'mostly', 'football', 'level', 'mornin', 'realize', 'angry', 'blah', 'limit', 'wanting', 'england', 'oooh', 'science', 'marathon', 'flat', 'clear', 'matt', 'ound', 'workout', 'easier', 'learning', 'cooking', 'hinks', 'juice', 'content', 'schedule', 'texas', 'error', 'honest', 'taylorswift13', 'entire', 'here', 'prayer', 'butt', 'allergy', 'cheap', 'meal', 'kept', 'brown', 'sandwich', 'breaking', 'mini', 'suggestion', 'land', 'dang', 'conversation', 'darn', 'chinese', 'cloud', 'reality', 'scene', 'option', 'table', 'expect', 'national', 'click', 'handle', 'pleasure', 'dumb', 'notice', 'grow', 'shitty', 'wing', 'pound', 'pair', 'british', 'fault', 'mitchelmusso', 'hole', 'orry', 'otherwise', 'attention', 'nightmare', 'arrived', 'trust', 'orange', 'strange', 'flying', 'oving', 'public', 'alex', 'lived', 'accident', 'drama', 'paris', 'arent', 'official', 'diet', 'jean', 'reminds', 'productive', 'driver', 'decide', 'interested', 'hehehe', 'ooks', 'cough', 'planned', 'liking', 'oooo', 'waste', 'married', 'main', 'anytime', 'blow', 'properly', 'posting', 'heavy', 'alarm', 'onto', 'cupcake', 'student', 'figured', 'painful', 'anging', 'daily', 'imma', 'beginning', 'appointment', 'harry', '2009', 'signed', 'ankle', 'slightly', 'comp', 'theme', 'crossed', 'updating', '2morrow', 'lease', 'goodness', 'othing', 'diversity', 'decision', 'deleted', 'contest', 'teach', 'twitpic', 'quit', 'pull', 'uncle', 'biggest', 'applications', 'painting', 'soul', 'status', 'fellow', 'catching', 'kicked', 'ruined', 'squarespace', 'spam', 'score', 'choose', 'letter', 'mmmm', 'jack', 'argh', 'amount', 'mistake', 'woohoo', 'yard', 'detail', 'sarah', 'sydney', 'oops', 'swim', 'macbook', 'extremely', 'boot', 'sushi', 'prefer', 'truly', 'blessed', 'quality', 'latest', 'attack', 'grey', 'transformer', 'total', 'older', 'aunt', 'rough', 'alive', 'human', 'tweeter', 'aying', 'trending', 'remembered', 'growing', 'commercial', 'thankyou', 'treat', 'managed', 'recently', 'mike', 'information', 'turning', 'woot', 'animal', 'pulled', 'ould', 'skype', 'quickly', 'montana', 'susan', 'killer', 'setting', 'shoulder', 'veryone', 'wifi', 'user', 'nooo', 'file', 'chick', 'everyones', 'anniversary', 'tennis', 'wash', 'sunburn', 'famous', 'recommendation', 'realised', 'queen', 'migraine', 'smart', 'hardly', 'massive', 'excuse', 'toast', 'research', 'upgrade', 'certain', 'program', 'hella', 'alking', 'lord', 'nasty', 'hadnt', 'smoke', 'access', 'chillin', 'guilty', 'revise', 'advice', 'crew', 'center', 'workin', 'performance', 'blocked', 'noooo', 'ireland', 'jesus', 'kitten', 'packed', 'lead', 'fuckin', 'healthy', 'third', 'coast', 'dirty', 'tooth', 'california', 'drag', 'classic', 'brand', 'massage', 'difference', 'hould', 'network', 'opening', 'otta', 'sugar', 'address', 'hearing', 'cali', 'thumb', 'ruin', 'dish', 'model', 'camping', 'insane', 'gross', 'neighbor', 'colour', 'reach', 'saved', 'opinion', 'effect', 'crack', 'mode', 'fashion', 'knowing', 'ohhh', 'presentation', 'haircut', 'winner', 'idol', 'term', 'burger', 'software', 'situation', 'action', 'dollar', 'period', 'bunny']\n"
     ]
    }
   ],
   "source": [
    "nr_of_words_to_use = 1500\n",
    "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['Pos'] + x[1]['Neg'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]\n",
    "print(popular_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute P(w|pos), P(w|neg) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_pos = {}\n",
    "P_w_given_neg = {}\n",
    "for word in popular_words:\n",
    "    P_w_given_pos[word] = (word_counter[word][\"Pos\"]/len([x for x in train_labels if x == 1]))/P_pos\n",
    "    P_w_given_neg[word] = (word_counter[word][\"Neg\"]/len([x for x in train_labels if x == 0]))/P_neg\n",
    "    # Calculate the two probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis'  : popular_words,\n",
    "    'P(pos)'   : P_pos,\n",
    "    'P(neg)'   : P_neg,\n",
    "    'P(w|pos)' : P_w_given_pos,\n",
    "    'P(w|neg)' : P_w_given_neg\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Positive or Negative using the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )* P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is Positive or Negative. i.e 'Pos' or 'Neg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(pos)' - class probabilities\n",
    "                                          'P(neg)' - class probabilities\n",
    "                                          'P(w|pos)' - conditional probabilities\n",
    "                                          'P(w|neg)' - conditional probabilities\n",
    "        \n",
    "        return: either 'Pos' or 'Neg'\n",
    "    \"\"\"\n",
    "# ... Code for classifying tweets using the naive bayes classifier\n",
    "\n",
    "    deconstructed_tweet = process_tweets(tweet)\n",
    "    deconstructed_tweet = convert_abbrev_in_text(deconstructed_tweet)\n",
    "\n",
    "    words_in_tweet = set(deconstructed_tweet.split())\n",
    "\n",
    "    pos_numerator = 1\n",
    "    neg_numerator = 1\n",
    "\n",
    "    for word in classifier_dict[\"basis\"]:\n",
    "        if word in words_in_tweet:\n",
    "            pos_numerator *= classifier_dict[\"P(w|pos)\"][word]\n",
    "            neg_numerator *= classifier_dict[\"P(w|neg)\"][word]\n",
    "        else:\n",
    "            pos_numerator *= 1 - classifier_dict[\"P(w|pos)\"][word]\n",
    "            neg_numerator *= 1 - classifier_dict[\"P(w|neg)\"][word]\n",
    "\n",
    "    pos_numerator *= classifier_dict[\"P(pos)\"]\n",
    "    neg_numerator *= classifier_dict[\"P(neg)\"]\n",
    "\n",
    "    return \"Pos\" if pos_numerator > neg_numerator else \"Neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet,label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet,classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_labels = test_labels.replace(0, \"Neg\", regex=True)\n",
    "final_test_labels = new_test_labels.replace(1, \"Pos\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6910\n"
     ]
    }
   ],
   "source": [
    "acc = test_classifier(classifier, test_tweets, final_test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional work\n",
    "\n",
    "In basic sentiment analysis classifications we have 3 classes \"Positive\", \"Negative\" and \"Neutral\". Although because it is challenging to create the \"Neutral\" class. Try to improve the accuracy by filtering the dataset from the perspective of removing words that indicate neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2db06c1aab9c62ea5648eba1e9f6aa4357642e01c7aa11d639b770a6a8c49d2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
